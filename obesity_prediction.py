# -*- coding: utf-8 -*-
"""Obesity_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x6kzgscPXPRXFy8TqIrVkhwdIw1S2_no

# Import Library
Import library yang dibutuhkan
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import cross_val_score
import numpy as np
import warnings

warnings.filterwarnings("ignore")

"""# Data Loading

Di sini kita download data dari kaagle menggunakan API dan membacanya

"""

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d mrsimple07/obesity-prediction
!unzip obesity-prediction.zip

df = pd.read_csv('obesity_data.csv')
df.head()

"""Mengecek jumlah data"""

df.shape

"""# EDA (Eksploratory Data Analysis)
Ini adalah proses analisis data yang digunakan untuk memahami struktur, pola, dan karakteristik data
"""

df.info()

"""#### cek informasi statistik"""

df.describe()

"""##### Cek nilai yang hilang dan data duplikat"""

print("Nilai Null :")
print(df.isnull().sum())

print(f"Nilai Duplikat:{df.duplicated().sum()}")

"""#### Mengecek dan menghapus outliers"""

numerik_cols = df.select_dtypes(include=['int64', 'float64']).columns

plt.figure(figsize=(15, 8))
for i, col in enumerate(numerik_cols, 1):
    plt.subplot(2, (len(numerik_cols) + 1)//2, i)
    sns.boxplot(y=df[col])
    plt.title(f'Boxplot {col}')
plt.tight_layout()
plt.show()

Q1 = df[numerik_cols].quantile(0.25)
Q3 = df[numerik_cols].quantile(0.75)
IQR = Q3 - Q1

outliers_iqr = ((df[numerik_cols] < (Q1 - 1.5 * IQR)) |
                (df[numerik_cols] > (Q3 + 1.5 * IQR)))

print("Jumlah outlier berdasarkan IQR:")
print(outliers_iqr.sum())

"""####  Melakukan Univariate Analysis

Mengecek fitur kategorikal dan numerik
"""

numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()

print("Fitur numerik:", numerical_features)
print("Fitur kategorikal:", categorical_features)

"""Cateorical Features"""

plt.figure(figsize=(6, 4))
sns.countplot(data=df, x='Gender')
plt.title('Distribusi Gender')
plt.show()

gender_dist = df['Gender'].value_counts(normalize=True) * 100
print("Proporsi Gender (%):\n", gender_dist.round(2))

plt.figure(figsize=(10, 6))
sns.displot(df['Age'], bins=30, aspect=1.5)

plt.title("Distribusi Usia", fontsize=16)
plt.xlabel("Usia", fontsize=14)
plt.ylabel("Frekuensi", fontsize=14)
plt.show()

sns.distplot(df['BMI'],rug=False)
plt.title("Distribusi BMI")
plt.show()

plt.figure(figsize=(12, 7))
sns.countplot(data=df, x='ObesityCategory', hue='Gender')

plt.title("Distribusi Obesity Category Berdasarkan Gender", fontsize=16)
plt.xlabel("Kategori Obesitas", fontsize=14)
plt.ylabel("Jumlah Individu", fontsize=14)

plt.show()

"""Numerical Features"""

df.hist(bins=50, figsize=(20,15))
plt.show()

"""# Data Preparation

Pada bagian ini, kita akan melakukan empat tahap persiapan data untuk mempersiapkan dataset sebelum digunakan dalam model machine learning. Pertama, kita akan melakukan encoding pada fitur kategori untuk mengubah data kategorikal menjadi format yang dapat dipahami oleh model. Kemudian, kita akan membagi dataset menjadi dua bagian, yaitu data pelatihan dan data pengujian, dengan menggunakan fungsi train_test_split dari library sklearn. Terakhir, kita akan melakukan standarisasi pada data agar semua fitur berada dalam skala yang sama, sehingga model dapat bekerja lebih optimal

#### Penanganan Missing Value dan Duplikat

Pada tahap Exploratory Data Analysis (EDA) sebelumnya, dilakukan pengecekan untuk memastikan tidak ada missing value maupun data duplikat dalam dataset, hasilnya data tidak di temukan missing value ataupun duplikat sehingga kita tidak perlu menangani data missing value ataupun duplikat

#### Handling Outliers

Outliers yang ditemukan dihapus atau disesuaikan nilainya agar tidak mempengaruhi model secara negatif
"""

filter_no_outlier = ~((df[numerik_cols] < (Q1 - 1.5 * IQR)) |
                      (df[numerik_cols] > (Q3 + 1.5 * IQR))).any(axis=1)

df_cleaned = df[filter_no_outlier].reset_index(drop=True)

df_cleaned.shape

"""setelah outlier dihapus, data menjadi 974

#### Encoding
"""

gender_encoder = LabelEncoder()
obesity_encoder = LabelEncoder()

df_cleaned['Gender'] = gender_encoder.fit_transform(df_cleaned['Gender'])
df_cleaned['ObesityCategory'] = obesity_encoder.fit_transform(df_cleaned['ObesityCategory'])

print("Mapping ObesityCategory:")
for i, label in enumerate(obesity_encoder.classes_):
    print(f"{i} = {label}")

df_cleaned.head()

df_cleaned.corr()['ObesityCategory']

correlation_matrix = df_cleaned.corr()

plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", linewidths=0.5)
plt.title("Heatmap Korelasi Antar Fitur")
plt.show()

"""beberapa fitur dengan ObesityCategory memiliki korelasi yang rendah, ini tidak berarti fitur-fitur tersebut tidak penting. Meskipun hubungan antara fitur dan target tampak lemah, fitur tersebut tetap bisa memberikan informasi yang berguna ketika digunakan dalam model prediksi

#### Train-Test-Split

Train-Test Split adalah proses pembagian dataset menjadi dua bagian: satu bagian untuk melatih model (training set) dan satu bagian lainnya untuk menguji kinerja model (testing set). Saya membagi dataset dengan proporsi 80:20, di mana 80% dari data digunakan untuk pelatihan dan 20% sisanya digunakan untuk pengujian
"""

X = df_cleaned.drop(columns='ObesityCategory')
y = df_cleaned['ObesityCategory']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Shape of X_train: {X_train.shape}")
print(f"Shape of X_test: {X_test.shape}")

"""#### Standarisasi

Standarisasi bertujuan untuk mengubah skala fitur numerik sehingga memiliki distribusi dengan rata-rata 0 dan standar deviasi 1
"""

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""# Model Development

#### Logistic Regression

Logistic Regression adalah model klasifikasi yang digunakan untuk memprediksi probabilitas kategori biner atau lebih
"""

model_lr = LogisticRegression()
model_lr.fit(X_train_scaled, y_train)
y_pred_lr = model_lr.predict(X_test)

"""training diatas dilakukan dengan parameter default

#### Random Forest

Random Forest adalah model ensemble yang menggabungkan banyak pohon keputusan (decision trees) untuk membuat prediksi yang lebih kuat dan mengurangi kemungkinan overfitting dibandingkan dengan decision tree
"""

model_rf = RandomForestClassifier(random_state=42)
model_rf.fit(X_train_scaled, y_train)
y_pred_rf = model_rf.predict(X_test_scaled)

"""training diatas dilakukan dengan parameter default

#### KNN

K-Nearest Neighbors (KNN) adalah algoritma yang berbasis pada pengukuran jarak antara data baru dan data yang ada. Prediksi untuk kelas baru didasarkan pada mayoritas kelas dari k-tetangga terdekat
"""

model_knn = KNeighborsClassifier(n_neighbors=10)
model_knn.fit(X_train_scaled, y_train)
y_pred_knn = model_knn.predict(X_test_scaled)

"""training diatas dilakukan dengan parameter 10 tetangga

# Evaluasi Model
"""

models = {
    'Logistic Regression': model_lr,
    'Random Forest': model_rf,
    'K-Nearest Neighbors': model_knn
}

for model_name, model in models.items():
    print(f"Evaluasi {model_name}")
    y_pred = model.predict(X_test_scaled)
    accuracy = accuracy_score(y_test, y_pred)

    print(f"Accuracy: {accuracy * 100:.2f}%")
    print("Classification Report:")
    print(classification_report(y_test, y_pred))

    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['True 0', 'True 1'])
    plt.title(f'Confusion Matrix: {model_name}')
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()

    print("\n")

"""Dalam evaluasi di atas, Random Forest menunjukkan kinerja terbaik dengan akurasi tertinggi, diikuti oleh Logistic Regression yang memiliki performa sangat baik, sementara K-Nearest Neighbors memiliki akurasi yang lebih rendah di banding yang lain"""